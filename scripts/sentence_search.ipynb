{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca722b0-d0b5-4443-82a3-dc0590e27168",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0130a4f6-cc24-4604-8172-2c9d0604f246",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_path = Path(\"..\", \"data\", \"raw\", \"SG sanctions on Russia.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4e68b5-833c-47d4-861d-e1e44addf62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    pd.read_excel(\n",
    "        raw_data_path,\n",
    "        sheet_name=\"Contents\",\n",
    "        parse_dates=[\"date\"],\n",
    "        usecols=[\n",
    "            \"id\",\n",
    "            \"source\",\n",
    "            \"title\",\n",
    "            \"content\",\n",
    "            \"date\",\n",
    "            \"parent\",\n",
    "            \"language\",\n",
    "            \"url\",\n",
    "            \"parent source identifier\",\n",
    "            \"domain\",\n",
    "            \"topics\",\n",
    "            \"image tags\",\n",
    "            \"sentiment\",\n",
    "            \"sentiment class\",\n",
    "            \"visibility\",\n",
    "            \"potential impressions\",\n",
    "            \"actual impressions\",\n",
    "            \"ave\",\n",
    "            \"city\",\n",
    "            \"country\",\n",
    "            \"gender\",\n",
    "            \"no. of comments\",\n",
    "            \"no. of likes\",\n",
    "            \"no. of shares\",\n",
    "            \"no. of retweets\",\n",
    "            \"no. of views\",\n",
    "            \"user name\",\n",
    "        ],\n",
    "    ).set_index(\"id\")\n",
    ")[lambda df: df[\"source\"] == \"Online News\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69804d4-6783-49f8-93af-88382c052627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentences we want sentence embeddings for\n",
    "titles = df[\"title\"].to_list()\n",
    "content = df[\"content\"].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1935f25-5160-4bdf-b12d-fe4e98e01619",
   "metadata": {},
   "source": [
    "## Load title/content embeddings, compute and save if not available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df738ae-6ef6-4729-8e09-15cb8f4b7ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"multi-qa-MiniLM-L6-cos-v1\")\n",
    "if \"content_embeddings.npy\" in set(map(str, Path().glob(\"*.npy\"))):\n",
    "    content_embeddings = np.load(\"content_embeddings.npy\")\n",
    "else:\n",
    "    content_embeddings = model.encode(content, batch_size=32, show_progress_bar=True)\n",
    "    np.save(\"content_embeddings.npy\", content_embeddings)\n",
    "    \n",
    "if \"title_embeddings.npy\" in set(map(str, Path().glob(\"*.npy\"))):\n",
    "    title_embeddings = np.load(\"title_embeddings.npy\")\n",
    "else:\n",
    "    title_embeddings = model.encode(titles, batch_size=32, show_progress_bar=True)\n",
    "    np.save(\"title_embeddings.npy\", title_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cfaf68-7788-43c6-adab-4166e1e15d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embeddings = content_embeddings\n",
    "display(model)\n",
    "sentence_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97325f2f-8eae-4ed0-8be2-0f185fbaa481",
   "metadata": {},
   "source": [
    "## Experiment with different search methods, try to create consistent API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79763654-af44-4aa9-a3d8-8893c0a7deb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "class SearchJaccard:\n",
    "    \"\"\"Jaccard similarity based on tokenized word sets\"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        model,\n",
    "        corpus: list[str],\n",
    "    ):\n",
    "        self.tokenizer = model.tokenizer\n",
    "        \n",
    "        # Save corpus for finding original text\n",
    "        self.df = pd.DataFrame(corpus, columns=[\"corpus\"])\n",
    "        self.df[\"token_set\"] = self.df[\"corpus\"].apply(lambda doc: set(self.tokenizer.tokenize(doc)))\n",
    "\n",
    "    def __call__(self, text: str, k: int):\n",
    "        text_token_set = set(self.tokenizer.tokenize(text))\n",
    "        similarity = self.df[\"token_set\"].apply(self.jaccard_similarity, B=text_token_set)\n",
    "        top_k = similarity.nlargest(k)\n",
    "        indexes = top_k.index.values\n",
    "        distances = top_k.values\n",
    "        \n",
    "        return distances, indexes\n",
    "    \n",
    "    def __getitem__(self, ids):\n",
    "        return self.df.loc[ids, \"corpus\"].to_list()\n",
    "    \n",
    "    # Reduce memory usage by storing corpus only in df, extract when needed\n",
    "    @property\n",
    "    def corpus(self):\n",
    "        return self.df[\"corpus\"].to_list()\n",
    "\n",
    "    @staticmethod\n",
    "    def jaccard_similarity(A, B):\n",
    "        #Find intersection of two sets\n",
    "        nominator = A.intersection(B)\n",
    "\n",
    "        #Find union of two sets\n",
    "        denominator = A.union(B)\n",
    "\n",
    "        #Take the ratio of sizes\n",
    "        similarity = len(nominator)/len(denominator)\n",
    "\n",
    "        return similarity\n",
    "\n",
    "    \n",
    "# jaccard_searcher = SearchJaccard(model=model, corpus=content)\n",
    "# d, i = jaccard_searcher(\"Russian government approved a list of countries and territories that are 'unfriendly'\", 20)\n",
    "# jaccard_searcher[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04145d4a-a89b-4bbc-9c94-1d37dd76b15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "class SearchFlatL2:\n",
    "    \"\"\"Exhaustive euclidean search on flat vector index\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        model,\n",
    "        sentence_embeddings,\n",
    "        corpus: list[str],\n",
    "    ):\n",
    "        self.model = model\n",
    "        \n",
    "        # Save corpus for finding original text\n",
    "        self.corpus = corpus\n",
    "\n",
    "        d = sentence_embeddings.shape[1]\n",
    "        \n",
    "        # IndexFlatL2 config\n",
    "        self.index = faiss.IndexFlatL2(d)\n",
    "        \n",
    "        # faiss indexing embeddings\n",
    "        self.index.add(sentence_embeddings)\n",
    "    \n",
    "    def __call__(self, text: str, k: int):\n",
    "        \n",
    "        xq = self.model.encode([text])\n",
    "        D, I = self.index.search(xq, k)\n",
    "        \n",
    "        return D[0], I[0]\n",
    "    \n",
    "    def __getitem__(self, ids):\n",
    "        return [self.corpus[i] for i in ids]\n",
    "    \n",
    "# flatl2_searcher = SearchFlatL2(model=model, sentence_embeddings=sentence_embeddings, corpus=content)\n",
    "# d, i = flatl2_searcher(\"Russian government approved a list of countries and territories that are 'unfriendly'\", k = 20)\n",
    "# flatl2_searcher[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029db454-bb18-47db-a622-15af9ccbe98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "class SearchHNSW:\n",
    "    \"\"\"HNSW Approximate Nearest Neighbour L2 search, use when require low memory usage\"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        model,\n",
    "        sentence_embeddings,\n",
    "        corpus: list[str],\n",
    "        M=64, # number of connections each vertex will have\n",
    "        ef_search=32, # depth of layers explored during search\n",
    "        ef_construction=64, # depth of layers explored during index construction\n",
    "    ):\n",
    "        self.model = model\n",
    "        \n",
    "        # Save corpus for finding original text\n",
    "        self.corpus = corpus\n",
    "\n",
    "        d = sentence_embeddings.shape[1]\n",
    "        \n",
    "        # HNSW config\n",
    "        self.index = faiss.IndexHNSWFlat(d, M)\n",
    "        self.index.hnsw.efConstruction = ef_construction\n",
    "        self.index.hnsw.efSearch = ef_search\n",
    "        \n",
    "        # faiss indexing embeddings\n",
    "        self.index.add(sentence_embeddings)\n",
    "    \n",
    "    def __call__(self, text: str, k: int):\n",
    "        \n",
    "        xq = self.model.encode([text])\n",
    "        D, I = self.index.search(xq, k)\n",
    "        \n",
    "        return D[0], I[0]\n",
    "    \n",
    "    def __getitem__(self, ids):\n",
    "        return [self.corpus[i] for i in ids]\n",
    "\n",
    "    \n",
    "# hnsw_searcher = SearchHNSW(model=model, sentence_embeddings=sentence_embeddings, corpus=content)\n",
    "# d, i = hnsw_searcher(\"Russian government approved a list of countries and territories that are 'unfriendly'\", k = 20)\n",
    "# hnsw_searcher[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4b483f-2af0-4e78-ab04-887f93034a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pynndescent import NNDescent\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "class SearchPyNN:\n",
    "    \"\"\"NNDescent\"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        model,\n",
    "        sentence_embeddings,\n",
    "        corpus: list[str],\n",
    "    ):\n",
    "        self.model = model\n",
    "        \n",
    "        # Save corpus for finding original text\n",
    "        self.corpus = corpus\n",
    "\n",
    "        d = sentence_embeddings.shape[1]\n",
    "        \n",
    "        # PyNNDescent indexing embeddings\n",
    "        self.index = NNDescent(sentence_embeddings)\n",
    "    \n",
    "    def __call__(self, text: str, k: int):\n",
    "        \n",
    "        xq = self.model.encode([text])\n",
    "        I, D = self.index.query(xq, k)\n",
    "        \n",
    "        return D[0], I[0]\n",
    "    \n",
    "    def __getitem__(self, ids):\n",
    "        return [self.corpus[i] for i in ids]\n",
    "\n",
    "    \n",
    "# pynn_searcher = SearchPyNN(model=model, sentence_embeddings=sentence_embeddings, corpus=content)\n",
    "# d, i = pynn_searcher(\"Russian government approved a list of countries and territories that are 'unfriendly'\", k = 20)\n",
    "# pynn_searcher[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9877fc3c-2fff-4202-9a17-25940de857e7",
   "metadata": {},
   "source": [
    "## Additional filters, separate from similarity\n",
    "\n",
    "To filter by metadata, \n",
    "1. Expand top_k params and do a post-filter. Drawback is that there could be too few results at the end.\n",
    "2. If filter param is discrete, can create and index for each combination of filter param\n",
    "3. Create an index on the fly for each configuration, but this will be the slowest and may negate any speed benefits.\n",
    "\n",
    "Explore filtering by date, by tokenized words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8e4bbb-2c66-4e39-8fd5-9498e6dc3c26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('news_matching')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "bf7832c6f6461a4b33e5448f0c9a9dd1b29b9bd3c65cb02a0f4bf75403fe7efa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
