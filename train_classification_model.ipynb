{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "598ea0e9-b868-423d-804b-b15987824cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\edmun\\Dropbox\\Internships\\GovTech\\Codebase\\news_matching\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import datetime\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from datasets import load_dataset, Features, Value\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.losses import CosineSimilarityLoss\n",
    "from setfit import SetFitModel, SetFitTrainer\n",
    "\n",
    "RAW_DATA_DIR = Path(\"data\", \"raw\")\n",
    "PROCESSED_DATA_DIR = Path(\"data\", \"processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "684b1389-dbb8-446f-8c42-0b85ee1da089",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data\\\\processed\\\\daily_scan_website_(sg)-web_articles-09_12_22-07_39.parquet',\n",
       " 'data\\\\processed\\\\daily_scan_website_(sg)-web_articles-09_16_22-09_06.parquet',\n",
       " 'data\\\\processed\\\\daily_scan_website_(sg)-web_articles-09_21_22-08_05.parquet',\n",
       " 'data\\\\processed\\\\daily_scan_website_(sg)-web_articles-09_23_22-08_00.parquet']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Use cleaned data + new labelled data for training\n",
    "train_files = [str(p) for p in PROCESSED_DATA_DIR.iterdir()]\n",
    "train_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3403f1d6-097d-4823-a362-d7ae12a4da80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.read_csv(\"all_tagged_articles - combined.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68ced40e-5850-4eea-9204-02908048f3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_df = pd.read_csv(\n",
    "    \"all_tagged_articles - combined.csv\", \n",
    "    usecols=[\"Published\", \"Headline\", \"Summary\", \"Theme\", \"New Index\"],\n",
    "    na_values=\"-\",\n",
    "    parse_dates=[\"Published\"],\n",
    ").rename(\n",
    "    lambda col_name: col_name.lower().replace(\" \", \"_\"), axis=\"columns\"\n",
    ").assign(label = lambda df: df[[\"theme\", \"new_index\"]].fillna(\"\").agg(' > '.join,axis=\"columns\"))\n",
    "\n",
    "t_df.to_parquet(\"test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbe22d3c-8d60-4de6-992c-a86d9b5c7b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10639 entries, 0 to 10638\n",
      "Data columns (total 6 columns):\n",
      " #   Column     Non-Null Count  Dtype         \n",
      "---  ------     --------------  -----         \n",
      " 0   published  10303 non-null  datetime64[ns]\n",
      " 1   headline   10639 non-null  object        \n",
      " 2   summary    7894 non-null   object        \n",
      " 3   theme      10639 non-null  object        \n",
      " 4   new_index  10639 non-null  object        \n",
      " 5   label      10639 non-null  object        \n",
      "dtypes: datetime64[ns](1), object(5)\n",
      "memory usage: 498.8+ KB\n"
     ]
    }
   ],
   "source": [
    "t_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60845d07-243b-4c0f-9c13-a0a2107bb20b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset parquet/default to C:/Users/edmun/.cache/huggingface/datasets/parquet/default-f88b40d0815b276c/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 1/1 [00:00<00:00, 989.92it/s]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 285.13it/s]\n",
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to C:/Users/edmun/.cache/huggingface/datasets/parquet/default-f88b40d0815b276c/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 55.24it/s]\n",
      "model_head.pkl not found on HuggingFace Hub, initialising classification head with random weights. You should TRAIN this model on a downstream task to use it for predictions and inference.\n",
      "Applying column mapping to training dataset\n",
      "Generating Training Pairs: 100%|██████████| 20/20 [00:15<00:00,  1.26it/s]\n",
      "***** Running training *****\n",
      "  Num examples = 425320\n",
      "  Num epochs = 1\n",
      "  Total optimization steps = 26583\n",
      "  Total train batch size = 16\n",
      "Iteration: 100%|██████████| 26583/26583 [51:45<00:00,  8.56it/s]\n",
      "Epoch: 100%|██████████| 1/1 [51:45<00:00, 3105.61s/it]\n",
      "c:\\Users\\edmun\\Dropbox\\Internships\\GovTech\\Codebase\\news_matching\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace with taxonomy\n",
    "\n",
    "# Scaffold for trial\n",
    "# df = pd.read_csv(\"tagged_articles.csv\", usecols=[\"Headline\", \"Theme\", \"New Index\", \"New Sub Index\", \"label\"]).rename(lambda col_name: col_name.lower().replace(\" \", \"_\"), axis=\"columns\")\n",
    "min_labels_list = t_df[\"label\"].value_counts()[lambda s: s>=2].index.to_list()\n",
    "\n",
    "# TODO: Replace with duckdb schema\n",
    "features = Features({\n",
    "    'published': Value('timestamp[ns]'),\n",
    "    'headline': Value('string'),\n",
    "    'summary': Value('string'),\n",
    "    'theme': Value('string'),\n",
    "    'new_index': Value('string'),\n",
    "    'label': Value('string'),\n",
    "})\n",
    "\n",
    "# TODO: Load real data when ready\n",
    "# dataset = load_dataset(\"parquet\", data_files={'train': train_files}, features=features)\n",
    "dataset = load_dataset(\"parquet\", data_files={'train': \"test.parquet\"}, features=features).filter(lambda row: row['label'] in min_labels_list)\n",
    "\n",
    "# # Fast train for testing\n",
    "# train_dataset = sample_dataset(dataset[\"train\"], label_column=\"label\", num_samples=4)\n",
    "train_dataset = dataset[\"train\"]\n",
    "\n",
    "# Load a SetFit model\n",
    "model = SetFitModel.from_pretrained(\n",
    "    \"sentence-transformers/multi-qa-MiniLM-L6-cos-v1\", \n",
    "    cache_dir=\"cached_models\",\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = SetFitTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    loss_class=CosineSimilarityLoss,\n",
    "    metric=\"accuracy\",\n",
    "    batch_size=16,\n",
    "    num_iterations=20, # The number of text pairs to generate for contrastive learning\n",
    "    num_epochs=1, # The number of epochs to use for contrastive learning\n",
    "    column_mapping={\"headline\": \"text\", \"label\": \"label\"} # Map dataset columns to text/label expected by trainer\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "trainer.model.save_pretrained(f\"trained_models/{datetime.date.today().isoformat()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
